# ollama create config-exaone-deep-2-4b -f exaone-deep-2-4b.Modelfile
# ollama create bb-config-exaone-deep-2-4b -f exaone-deep-2-4b.Modelfile --system "$(cat optional_system_prompt.md)"
FROM exaone-deep:2.4b
# As EXAONE, I am a large language model developed by LG AI Research
# Author: NAVER
# Size on disk: 
# Size running: 10 GB
# ollama show config-exaone-deep-2-4b && ollama run config-exaone-deep-2-4b "what llm model are you"
#  architecture        exaone    
#  parameters          2.7B      
#  context length      32768     
#  embedding length    2560      
#  quantization        Q4_K_M    

PARAMETER temperature 0.6
PARAMETER top_p 0.95
PARAMETER num_ctx 32768
