# ollama create config-unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL -f unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL.Modelfile
# ollama create bb-config-unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL -f unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL.Modelfile --system "$(cat optional_system_prompt.md)"
FROM hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q3_K_XL
# I am Qwen, a large-scale language model developed by Alibaba Cloud.
# Author: unsloth
# Size on disk: 
# Size running: 56 GB    100% CPU
# ollama show config-unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL && ollama run config-unsloth-Qwen3-30B-A3B-Instruct-2507-GGUF-Q3_K_XL "what llm model are you"
#  architecture        qwen3moe    
#  parameters          30.5B       
#  context length      262144      
#  embedding length    2048        
#  quantization        Q3_K_M      

#  Number of Experts: 128
#  Number of Activated Experts: 8
#  non-thinking
#  context_length 	262144
#  output length of 16,384

PARAMETER num_ctx 262144
PARAMETER num_predict 16384
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER top_k 20
PARAMETER min_p 0.0
