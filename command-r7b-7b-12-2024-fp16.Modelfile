# ollama create config-command-r7b-7b-12-2024-fp16 -f command-r7b-7b-12-2024-fp16.Modelfile
# ollama create bb-config-command-r7b-7b-12-2024-fp16 -f command-r7b-7b-12-2024-fp16.Modelfile --system "$(cat optional_system_prompt.md)"
FROM command-r7b:7b-12-2024-fp16
# Author: Cohere
# Size on disk: 
# Size running: 18 GB
#  architecture        cohere2    
#  parameters          8.0B       
#  context length      8192       
#  embedding length    4096       
#  quantization        F16        
#  Error: 500 Internal Server Error: llama runner process has terminated: cudaMalloc failed: out of memory
#  ggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 2638217216

PARAMETER num_predict 100
PARAMETER temperature 0.3
PARAMETER stop "<|START_OF_TURN_TOKEN|>"
PARAMETER stop "<|END_OF_TURN_TOKEN|>"
PARAMETER stop "<|END_RESPONSE|>"
PARAMETER num_ctx 131072
PARAMETER top_k 40
PARAMETER top_p 0.95
PARAMETER min_p 0.05
PARAMETER typical_p 1.0

