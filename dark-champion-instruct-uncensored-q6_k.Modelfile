# ollama create config-dark-champion-instruct-uncensored-q6_k -f dark-champion-instruct-uncensored-q6_k.Modelfile
# ollama create bb-config-dark-champion-instruct-uncensored-q6_k -f dark-champion-instruct-uncensored-q6_k.Modelfile --system "$(cat optional_system_prompt.md)"
FROM hf.co/DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B-GGUF:Q6_K
# I am a large language model (LLM) based on the transformer architecture
# Author: DavidAU
# Size on disk: 
# Size running: 21 GB
#  architecture        llama     
#  parameters          18.4B     
#  context length      131072    
#  embedding length    3072      
#  quantization        Q6_K      

# roo:
#  started architect
#  would not read files
#  asks too many questions

PARAMETER temperature 0.3
PARAMETER top_p 0.95
PARAMETER num_ctx 32768
