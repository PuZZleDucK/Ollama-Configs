# ollama create config-llama3 -f llama3.Modelfile
# ollama create bb-config-llama3 -f llama3.Modelfile --system "$(cat optional_system_prompt.md)"
FROM llama3
# 
# Author: 
# Size on disk: 
# Size running: 
# ollama show config-llama3 && ollama run config-llama3 "what llm model are you"

#  max_new_tokens or max_tokens => num_predict # default 128
#  context_length => num_ctx
#  do_sample -> remove
#  repetition_penalty -> repeat_penalty
#  128k => 131072 - 32k => 32768
#  PARAMETER stop -> one per line
PARAMETER temperature 0.3
PARAMETER top_p 0.95
PARAMETER num_ctx 131072
PARAMETER num_predict 100
PARAMETER stop "<|START_OF_TURN_TOKEN|>"
PARAMETER stop "<|END_OF_TURN_TOKEN|>"
PARAMETER stop "<|END_RESPONSE|>"
PARAMETER top_k 40
PARAMETER min_p 0.05
PARAMETER typical_p 1.0
PARAMETER repeat_penalty 1.03
PARAMETER repeat_last_n
PARAMETER presence_penalty 1.5

# temperature	Randomness / creativity	0 – 1.2 (0 = deterministic)
# top_p	Nucleus sampling cutoff	0.7 – 0.95
# top_k	Consider only top‑k tokens	20 – 100
# repeat_penalty	Discourage repetition	1.1 – 1.5
# num_predict	Max tokens to generate	128 (‑1 = unlimited)
# num_ctx

#  SYSTEM You are a concise coding assistant.
#  TEMPLATE """{{ if .System }}{{ .System }} {{ end }}<!-- original --> {{ .Prompt }}"""
#    #  • {{ .System }} – the system prompt (may be empty)
#    #  • {{ .Prompt }} – the user prompt (single‑turn generate) or the last user message (chat)
#    #  • {{ .Messages }} / {{ range .Messages }} – an array of all prior chat turns (only when you call /api/chat)
#    #  • {{ .Response }} – marker indicating where the model’s answer should start (everything after it is thrown away during generation).
#    #  Wrap sections with {{ if .System }} … {{ end }} or {{ if .Prompt }} so the block disappears when the variable is empty.

#  multiline template
#  TEMPLATE """
#  <|system|>
#  {system_message}</s>
#  <|user|>
#  {prompt}</s>
#  <|assistant|>
#  """

